{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from scipy.special import softmax\n",
    "import scipy.stats as ss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,recall_score,precision_score,precision_recall_curve,f1_score,auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "skf2 = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model = model\n",
    "            self.val_loss_min = val_loss\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.best_model = model\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Module = 16\n",
    "\n",
    "class mtlAttention(nn.Module):\n",
    "    def __init__(self, In_Nodes1,In_Nodes2, Modules):\n",
    "        super(mtlAttention, self).__init__()\n",
    "        self.Modules = Modules\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.task1_FC1_x = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "        self.task1_FC1_y = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "\n",
    "        self.task2_FC1_x = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "        self.task2_FC1_y = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "            \n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.task1_FC2 =nn.Sequential(nn.Linear(Modules*2, 16),nn.ReLU())\n",
    "        self.task2_FC2 = nn.Sequential(nn.Linear(Modules*2, 16),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC3 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "        self.task2_FC3 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward_one(self,xg,xm):\n",
    "        xg_x = self.task1_FC1_x(xg)\n",
    "        xm_x = self.task2_FC1_x(xm)\n",
    "        xg_y = self.task1_FC1_y(xg)         \n",
    "        xm_y =self.task2_FC1_y(xm)\n",
    "\n",
    "        xg = torch.cat([xg_x.reshape(-1,1,self.Modules),xg_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        xm = torch.cat([xm_x.reshape(-1,1,self.Modules),xm_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        \n",
    "        norm  = torch.norm(xg, dim=1, keepdim=True)\n",
    "        xg = xg.div(norm)\n",
    "        \n",
    "        norm  = torch.norm(xm, dim=1, keepdim=True)\n",
    "        xm = xm.div(norm)\n",
    "        \n",
    "        energy =  torch.bmm(xg.reshape(-1,2,self.Modules).permute(0,2,1) ,xm.reshape(-1,2,self.Modules))\n",
    "        attention1 = self.softmax(energy.permute(0,2,1)).permute(0,2,1) \n",
    "        attention2 = self.softmax(energy).permute(0,2,1)\n",
    "        \n",
    "        xg_value = torch.bmm(xg,attention1) \n",
    "        xm_value = torch.bmm(xm,attention2)\n",
    "\n",
    "        xg = xg_value.view(-1,self.Modules*2)\n",
    "        xm =xm_value.view(-1,self.Modules*2)\n",
    "        \n",
    "        xg = self.task1_FC2(xg)\n",
    "        xm = self.task2_FC2(xm) \n",
    "        xg = self.task1_FC3(xg)\n",
    "        xm = self.task2_FC3(xm)\n",
    "        \n",
    "        return xg,xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Module = 32\n",
    "\n",
    "class mtlAttention(nn.Module):\n",
    "    def __init__(self, In_Nodes1,In_Nodes2, Modules):\n",
    "        super(mtlAttention, self).__init__()\n",
    "        self.Modules = Modules\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.task1_FC1_x = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "        self.task1_FC1_y = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "\n",
    "        self.task2_FC1_x = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "        self.task2_FC1_y = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "            \n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.task1_FC2 =nn.Sequential(nn.Linear(Modules*2, 32),nn.ReLU())\n",
    "        self.task2_FC2 = nn.Sequential(nn.Linear(Modules*2, 32),nn.ReLU())\n",
    "\n",
    "        self.task1_FC3 =nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        self.task2_FC3 = nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC4 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "        self.task2_FC4 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward_one(self,xg,xm):\n",
    "        xg_x = self.task1_FC1_x(xg)\n",
    "        xm_x = self.task2_FC1_x(xm)\n",
    "        xg_y = self.task1_FC1_y(xg)         \n",
    "        xm_y =self.task2_FC1_y(xm)\n",
    "\n",
    "        xg = torch.cat([xg_x.reshape(-1,1,self.Modules),xg_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        xm = torch.cat([xm_x.reshape(-1,1,self.Modules),xm_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        \n",
    "        norm  = torch.norm(xg, dim=1, keepdim=True)\n",
    "        xg = xg.div(norm)\n",
    "        \n",
    "        norm  = torch.norm(xm, dim=1, keepdim=True)\n",
    "        xm = xm.div(norm)\n",
    "        \n",
    "        energy =  torch.bmm(xg.reshape(-1,2,self.Modules).permute(0,2,1) ,xm.reshape(-1,2,self.Modules))\n",
    "        attention1 = self.softmax(energy.permute(0,2,1)).permute(0,2,1) \n",
    "        attention2 = self.softmax(energy).permute(0,2,1)\n",
    "        \n",
    "        xg_value = torch.bmm(xg,attention1) \n",
    "        xm_value = torch.bmm(xm,attention2)\n",
    "\n",
    "        xg = xg_value.view(-1,self.Modules*2)\n",
    "        xm =xm_value.view(-1,self.Modules*2)\n",
    "        \n",
    "        xg = self.task1_FC2(xg)\n",
    "        xm = self.task2_FC2(xm) \n",
    "        xg = self.task1_FC3(xg)\n",
    "        xm = self.task2_FC3(xm)\n",
    "        xg = self.task1_FC4(xg)\n",
    "        xm = self.task2_FC4(xm)\n",
    "        \n",
    "        return xg,xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Module = 64\n",
    "\n",
    "class mtlAttention(nn.Module):\n",
    "    def __init__(self, In_Nodes1,In_Nodes2, Modules):\n",
    "        super(mtlAttention, self).__init__()\n",
    "        self.Modules = Modules\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.task1_FC1_x = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "        self.task1_FC1_y = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "\n",
    "        self.task2_FC1_x = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "        self.task2_FC1_y = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "            \n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.task1_FC2 =nn.Sequential(nn.Linear(Modules*2, 64),nn.ReLU())\n",
    "        self.task2_FC2 = nn.Sequential(nn.Linear(Modules*2, 64),nn.ReLU())\n",
    "\n",
    "        self.task1_FC3 =nn.Sequential(nn.Linear(64, 32),nn.ReLU())\n",
    "        self.task2_FC3 = nn.Sequential(nn.Linear(64, 32),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC4 =nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        self.task2_FC4 = nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC5 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "        self.task2_FC5 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward_one(self,xg,xm):\n",
    "        xg_x = self.task1_FC1_x(xg)\n",
    "        xm_x = self.task2_FC1_x(xm)\n",
    "        xg_y = self.task1_FC1_y(xg)         \n",
    "        xm_y =self.task2_FC1_y(xm)\n",
    "\n",
    "        xg = torch.cat([xg_x.reshape(-1,1,self.Modules),xg_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        xm = torch.cat([xm_x.reshape(-1,1,self.Modules),xm_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        \n",
    "        norm  = torch.norm(xg, dim=1, keepdim=True)\n",
    "        xg = xg.div(norm)\n",
    "        \n",
    "        norm  = torch.norm(xm, dim=1, keepdim=True)\n",
    "        xm = xm.div(norm)\n",
    "        \n",
    "        energy =  torch.bmm(xg.reshape(-1,2,self.Modules).permute(0,2,1) ,xm.reshape(-1,2,self.Modules))\n",
    "        attention1 = self.softmax(energy.permute(0,2,1)).permute(0,2,1) \n",
    "        attention2 = self.softmax(energy).permute(0,2,1)\n",
    "        \n",
    "        xg_value = torch.bmm(xg,attention1) \n",
    "        xm_value = torch.bmm(xm,attention2)\n",
    "\n",
    "        xg = xg_value.view(-1,self.Modules*2)\n",
    "        xm =xm_value.view(-1,self.Modules*2)\n",
    "        \n",
    "        xg = self.task1_FC2(xg)\n",
    "        xm = self.task2_FC2(xm) \n",
    "        xg = self.task1_FC3(xg)\n",
    "        xm = self.task2_FC3(xm)\n",
    "        xg = self.task1_FC4(xg)\n",
    "        xm = self.task2_FC4(xm)\n",
    "        xg = self.task1_FC5(xg)\n",
    "        xm = self.task2_FC5(xm)\n",
    "        \n",
    "        return xg,xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Module =128\n",
    "\n",
    "class mtlAttention(nn.Module):\n",
    "    def __init__(self, In_Nodes1,In_Nodes2, Modules):\n",
    "        super(mtlAttention, self).__init__()\n",
    "        self.Modules = Modules\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.task1_FC1_x = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "        self.task1_FC1_y = nn.Linear(In_Nodes1, Modules,bias=False)\n",
    "\n",
    "        self.task2_FC1_x = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "        self.task2_FC1_y = nn.Linear(In_Nodes2, Modules,bias=False)\n",
    "            \n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.task1_FC2 =nn.Sequential(nn.Linear(Modules*2, 128),nn.ReLU())\n",
    "        self.task2_FC2 = nn.Sequential(nn.Linear(Modules*2, 128),nn.ReLU())\n",
    "\n",
    "        self.task1_FC3 =nn.Sequential(nn.Linear(128, 64),nn.ReLU())\n",
    "        self.task2_FC3 = nn.Sequential(nn.Linear(128, 64),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC4 =nn.Sequential(nn.Linear(64, 32),nn.ReLU())\n",
    "        self.task2_FC4 = nn.Sequential(nn.Linear(64, 32),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC5 =nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        self.task2_FC5 = nn.Sequential(nn.Linear(32, 16),nn.ReLU())\n",
    "        \n",
    "        self.task1_FC6 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "        self.task2_FC6 = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward_one(self,xg,xm):\n",
    "        xg_x = self.task1_FC1_x(xg)\n",
    "        xm_x = self.task2_FC1_x(xm)\n",
    "        xg_y = self.task1_FC1_y(xg)         \n",
    "        xm_y =self.task2_FC1_y(xm)\n",
    "\n",
    "        xg = torch.cat([xg_x.reshape(-1,1,self.Modules),xg_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        xm = torch.cat([xm_x.reshape(-1,1,self.Modules),xm_y.reshape(-1,1,self.Modules)], dim=1)\n",
    "        \n",
    "        norm  = torch.norm(xg, dim=1, keepdim=True)\n",
    "        xg = xg.div(norm)\n",
    "        \n",
    "        norm  = torch.norm(xm, dim=1, keepdim=True)\n",
    "        xm = xm.div(norm)\n",
    "        \n",
    "        energy =  torch.bmm(xg.reshape(-1,2,self.Modules).permute(0,2,1) ,xm.reshape(-1,2,self.Modules))\n",
    "        attention1 = self.softmax(energy.permute(0,2,1)).permute(0,2,1) \n",
    "        attention2 = self.softmax(energy).permute(0,2,1)\n",
    "        \n",
    "        xg_value = torch.bmm(xg,attention1) \n",
    "        xm_value = torch.bmm(xm,attention2)\n",
    "\n",
    "        xg = xg_value.view(-1,self.Modules*2)\n",
    "        xm =xm_value.view(-1,self.Modules*2)\n",
    "        \n",
    "        xg = self.task1_FC2(xg)\n",
    "        xm = self.task2_FC2(xm) \n",
    "        xg = self.task1_FC3(xg)\n",
    "        xm = self.task2_FC3(xm)\n",
    "        xg = self.task1_FC4(xg)\n",
    "        xm = self.task2_FC4(xm)\n",
    "        xg = self.task1_FC5(xg)\n",
    "        xm = self.task2_FC5(xm)\n",
    "        xg = self.task1_FC6(xg)\n",
    "        xm = self.task2_FC6(xm)\n",
    "        \n",
    "        return xg,xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = \"./mRNA.csv\"\n",
    "x2 = \"./meth.csv\"\n",
    "adj_file = \"./bipartite.csv\"\n",
    "label = \"./label.csv\"\n",
    "\n",
    "x1 = pd.read_csv(x1,index_col=0,delimiter=',')\n",
    "x2 = pd.read_csv(x2, index_col=0, delimiter=',')\n",
    "adj = pd.read_csv(adj_file, index_col=0)\n",
    "xy, x_ind, y_ind = np.intersect1d(x1.columns, x2.columns, return_indices=True)\n",
    "#_, x_ind1, y_ind1 = np.intersect1d(x2.index, adj.columns, return_indices=True)\n",
    "#xy1, x_ind2, y_ind2 = np.intersect1d(x1.index, adj.index, return_indices=True)\n",
    "\n",
    "x1 = x1.iloc[:, x_ind]\n",
    "x2 = x2.iloc[:, y_ind]\n",
    "\n",
    "x1 = x1.fillna(0)\n",
    "x2 = x2.fillna(0)\n",
    "\n",
    "data = pd.read_csv(label, delimiter=',', index_col=0)\n",
    "xy, x_ind, y_ind = np.intersect1d(x1.columns, data.index, return_indices=True)\n",
    "x1 = x1.iloc[:, x_ind]\n",
    "x2 = x2.iloc[:, x_ind]\n",
    "y = data.iloc[y_ind, :].astype(str)\n",
    "# y[y=='Positive']=1\n",
    "# y[y=='Negative']=0\n",
    "labels = np.array(y).astype(np.float32)\n",
    "# ER = 0 / TN = 1\n",
    "labels = labels[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-002daccb3cdf>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-9-002daccb3cdf>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.6649, BCE_task1; 1.3981, BCE_task2; 1.2668\n",
      "Epoch [200/500000], Loss: 2.6157, BCE_task1; 1.3703, BCE_task2; 1.2453\n",
      "Epoch [400/500000], Loss: 2.5923, BCE_task1; 1.3587, BCE_task2; 1.2335\n",
      "Epoch [600/500000], Loss: 2.5661, BCE_task1; 1.3469, BCE_task2; 1.2193\n",
      "Epoch [800/500000], Loss: 2.5263, BCE_task1; 1.3322, BCE_task2; 1.1941\n",
      "Epoch [1000/500000], Loss: 2.4812, BCE_task1; 1.3158, BCE_task2; 1.1654\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.862\n",
      "F1_task1 0.926, F1_task2 0.926\n",
      "AUC_task1 0.600, AUC_task2 0.590\n",
      "time : 30.717185974121094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-002daccb3cdf>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-9-002daccb3cdf>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.8133, BCE_task1; 1.2541, BCE_task2; 1.5592\n",
      "Epoch [200/500000], Loss: 2.7496, BCE_task1; 1.2142, BCE_task2; 1.5354\n",
      "Epoch [400/500000], Loss: 2.7299, BCE_task1; 1.2010, BCE_task2; 1.5289\n",
      "Epoch [600/500000], Loss: 2.7130, BCE_task1; 1.1882, BCE_task2; 1.5248\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.138\n",
      "F1_task1 0.926, F1_task2 0.000\n",
      "AUC_task1 0.330, AUC_task2 0.320\n",
      "time : 17.0046284198761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-002daccb3cdf>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-9-002daccb3cdf>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.7146, BCE_task1; 1.1897, BCE_task2; 1.5249\n",
      "Epoch [200/500000], Loss: 2.6734, BCE_task1; 1.1695, BCE_task2; 1.5038\n",
      "Epoch [400/500000], Loss: 2.6588, BCE_task1; 1.1600, BCE_task2; 1.4987\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.138\n",
      "F1_task1 0.926, F1_task2 0.000\n",
      "AUC_task1 0.560, AUC_task2 0.540\n",
      "time : 13.962572574615479\n",
      "Epoch [1/500000], Loss: 2.9770, BCE_task1; 1.5129, BCE_task2; 1.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-002daccb3cdf>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-9-002daccb3cdf>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/500000], Loss: 2.9109, BCE_task1; 1.4802, BCE_task2; 1.4306\n",
      "Epoch [400/500000], Loss: 2.8848, BCE_task1; 1.4693, BCE_task2; 1.4155\n",
      "Epoch [600/500000], Loss: 2.8508, BCE_task1; 1.4555, BCE_task2; 1.3953\n",
      "Epoch [800/500000], Loss: 2.8086, BCE_task1; 1.4390, BCE_task2; 1.3696\n",
      "Epoch [1000/500000], Loss: 2.7647, BCE_task1; 1.4257, BCE_task2; 1.3390\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.138, ACC_task2 0.862\n",
      "F1_task1 0.000, F1_task2 0.920\n",
      "AUC_task1 0.630, AUC_task2 0.620\n",
      "time : 26.41229820251465\n",
      "Epoch [1/500000], Loss: 2.8029, BCE_task1; 1.3842, BCE_task2; 1.4187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-002daccb3cdf>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-9-002daccb3cdf>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [200/500000], Loss: 2.7365, BCE_task1; 1.3492, BCE_task2; 1.3874\n",
      "Epoch [400/500000], Loss: 2.7086, BCE_task1; 1.3364, BCE_task2; 1.3722\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.759\n",
      "F1_task1 0.926, F1_task2 0.863\n",
      "AUC_task1 0.330, AUC_task2 0.520\n",
      "time : 15.877793312072754\n",
      "Task1 AUC: 0.490, Task2 AUC: 0.518\n"
     ]
    }
   ],
   "source": [
    "auc1 = []\n",
    "auc2 = []\n",
    "for fold_ in range(1,6):\n",
    "    train_index = pd.read_csv('./train_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    valid_index = pd.read_csv('./valid_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    test_index = pd.read_csv('./test_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    \n",
    "    train_index = np.array(train_index).reshape(-1).astype(int)\n",
    "    valid_index = np.array(valid_index).reshape(-1).astype(int)\n",
    "    test_index = np.array(test_index).reshape(-1).astype(int)\n",
    "    \n",
    "    Xg_train = x1.transpose().iloc[train_index,:].values\n",
    "    Xg_valid = x1.transpose().iloc[valid_index,:].values\n",
    "    Xg_test = x1.transpose().iloc[test_index,:].values\n",
    "\n",
    "    Xm_train = x2.transpose().iloc[train_index,:].values\n",
    "    Xm_valid = x2.transpose().iloc[valid_index,:].values\n",
    "    Xm_test = x2.transpose().iloc[test_index,:].values\n",
    "\n",
    "    y_train = labels[train_index]\n",
    "    y_valid = labels[valid_index]\n",
    "    y_test = labels[test_index]\n",
    "    \n",
    "    start = time.time()\n",
    "    earlyStoppingPatience = 100\n",
    "    learningRate = 0.000005\n",
    "    weightDecay = 0.001\n",
    "    num_epochs = 500000 \n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_valid = y_valid.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    Xg = torch.tensor(Xg_train, dtype=torch.float32).cuda()\n",
    "    Xm = torch.tensor(Xm_train, dtype=torch.float32).cuda()\n",
    "    \n",
    "    Xg_valid = torch.tensor(Xg_valid, dtype=torch.float32).cuda()\n",
    "    Xm_valid = torch.tensor(Xm_valid, dtype=torch.float32).cuda()\n",
    "    \n",
    "\n",
    "    Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
    "    Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n",
    "\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).cuda()\n",
    "    y_v = torch.tensor(y_valid, dtype=torch.float32).cuda()\n",
    "\n",
    "    ds = TensorDataset(Xg, Xm,y)\n",
    "    dv = TensorDataset(Xg_valid,Xm_valid,y_v)\n",
    "    \n",
    "    loader  = DataLoader(ds, batch_size=y_train.shape[0],shuffle=True)\n",
    "    loader_v = DataLoader(dv, batch_size=y_valid.shape[0],shuffle=True)\n",
    "\n",
    "   \n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    In_Nodes1 = Xg_train.shape[1] \n",
    "    In_Nodes2 = Xm_train.shape[1]\n",
    "\n",
    "    # mtlAttention(In_Nodes1,In_Nodes2, # of module)\n",
    "    net = mtlAttention(In_Nodes1,In_Nodes2,32)\n",
    "    net = net.to(device)\n",
    "    early_stopping = EarlyStopping(patience=earlyStoppingPatience, verbose=False)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    \n",
    "    train_losses =[]\n",
    "    valid_losses = [] \n",
    "    \n",
    "    for epoch in (range(num_epochs)):\n",
    "        running_loss1 = 0.0\n",
    "        running_loss2 = 0.0\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            xg,xm, y = data\n",
    "            output1,output2 = net.forward_one(xg,xm)\n",
    "            output1  = output1.squeeze()\n",
    "            output2  = output2.squeeze()\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output1, y) + loss_fn(output2, y) \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "            running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "            train_losses.append(running_loss1+running_loss2)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for i, data in enumerate(loader_v,0):\n",
    "                xg,xm, y = data\n",
    "                output1,output2 = net.forward_one(xg,xm)\n",
    "                output1  = output1.squeeze()\n",
    "                output2  = output2.squeeze()\n",
    "                loss = loss_fn(output1, y) + loss_fn(output2, y)\n",
    "                running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                valid_losses.append(running_loss1+running_loss2)\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        \n",
    "        early_stopping(valid_loss, net)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            print(\"--------------------------------------------------------------------------------------------------\")\n",
    "            break\n",
    "\n",
    "        if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "            if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.4f}, BCE_task1; {:.4f}, BCE_task2; {:.4f}'.format(epoch+1,num_epochs, running_loss1+running_loss2,running_loss1,running_loss2))\n",
    "\n",
    "    ### Test\n",
    "    best_net = early_stopping.best_model\n",
    "    test1,test2 = best_net.forward_one(Xg_test.clone().detach(),Xm_test.clone().detach())\n",
    "    test1 = test1.cpu().detach().numpy()\n",
    "    test2 = test2.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    print (\"ACC_task1 %.3f, ACC_task2 %.3f\" %(accuracy_score(list(y_test),np.where(test1 > 0.5, 1, 0) ),accuracy_score(list(y_test),np.where(test2 > 0.5, 1, 0))))\n",
    "    print (\"F1_task1 %.3f, F1_task2 %.3f\" %(f1_score(list(y_test),np.where(test1 > 0.5, 1, 0)),f1_score(list(y_test),np.where(test2 > 0.5, 1, 0))))\n",
    "    print (\"AUC_task1 %.3f, AUC_task2 %.3f\" %(roc_auc_score(y_test.reshape(-1),test1),roc_auc_score(y_test.reshape(-1),test2)))\n",
    "    print(\"time :\", time.time() - start)\n",
    "    auc1.append(roc_auc_score(y_test.reshape(-1),test1))\n",
    "    auc2.append(roc_auc_score(y_test.reshape(-1),test2))\n",
    "print(\"Task1 AUC: %.3f, Task2 AUC: %.3f\" %(sum(auc1)/len(auc1), sum(auc2)/len(auc2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4f065e543b06>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-10-4f065e543b06>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.8898, BCE_task1; 1.4431, BCE_task2; 1.4467\n",
      "Epoch [200/500000], Loss: 2.8508, BCE_task1; 1.4143, BCE_task2; 1.4365\n",
      "Epoch [400/500000], Loss: 2.8264, BCE_task1; 1.3997, BCE_task2; 1.4267\n",
      "Epoch [600/500000], Loss: 2.7983, BCE_task1; 1.3821, BCE_task2; 1.4162\n",
      "Epoch [800/500000], Loss: 2.7642, BCE_task1; 1.3595, BCE_task2; 1.4046\n",
      "Epoch [1000/500000], Loss: 2.7235, BCE_task1; 1.3320, BCE_task2; 1.3916\n",
      "Epoch [1200/500000], Loss: 2.6779, BCE_task1; 1.3015, BCE_task2; 1.3763\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.724\n",
      "F1_task1 0.926, F1_task2 0.833\n",
      "AUC_task1 0.550, AUC_task2 0.480\n",
      "time : 43.3552508354187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4f065e543b06>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-10-4f065e543b06>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 3.1319, BCE_task1; 1.6157, BCE_task2; 1.5161\n",
      "Epoch [200/500000], Loss: 3.0588, BCE_task1; 1.5891, BCE_task2; 1.4697\n",
      "Epoch [400/500000], Loss: 3.0211, BCE_task1; 1.5762, BCE_task2; 1.4450\n",
      "Epoch [600/500000], Loss: 2.9895, BCE_task1; 1.5622, BCE_task2; 1.4274\n",
      "Epoch [800/500000], Loss: 2.9621, BCE_task1; 1.5497, BCE_task2; 1.4124\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.138, ACC_task2 0.138\n",
      "F1_task1 0.000, F1_task2 0.000\n",
      "AUC_task1 0.150, AUC_task2 0.200\n",
      "time : 29.23701310157776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4f065e543b06>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-10-4f065e543b06>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.9783, BCE_task1; 1.6028, BCE_task2; 1.3754\n",
      "Epoch [200/500000], Loss: 2.8891, BCE_task1; 1.5470, BCE_task2; 1.3421\n",
      "Epoch [400/500000], Loss: 2.8545, BCE_task1; 1.5272, BCE_task2; 1.3274\n",
      "Epoch [600/500000], Loss: 2.8162, BCE_task1; 1.5080, BCE_task2; 1.3082\n",
      "Epoch [800/500000], Loss: 2.7800, BCE_task1; 1.4936, BCE_task2; 1.2864\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.138, ACC_task2 0.862\n",
      "F1_task1 0.000, F1_task2 0.926\n",
      "AUC_task1 0.730, AUC_task2 0.790\n",
      "time : 30.982171535491943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4f065e543b06>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-10-4f065e543b06>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 2.9043, BCE_task1; 1.3825, BCE_task2; 1.5219\n",
      "Epoch [200/500000], Loss: 2.8636, BCE_task1; 1.3735, BCE_task2; 1.4901\n",
      "Epoch [400/500000], Loss: 2.8486, BCE_task1; 1.3676, BCE_task2; 1.4810\n",
      "Epoch [600/500000], Loss: 2.8319, BCE_task1; 1.3607, BCE_task2; 1.4712\n",
      "Epoch [800/500000], Loss: 2.8121, BCE_task1; 1.3527, BCE_task2; 1.4594\n",
      "Epoch [1000/500000], Loss: 2.7895, BCE_task1; 1.3427, BCE_task2; 1.4467\n",
      "Epoch [1200/500000], Loss: 2.7588, BCE_task1; 1.3284, BCE_task2; 1.4304\n",
      "Epoch [1400/500000], Loss: 2.7195, BCE_task1; 1.3078, BCE_task2; 1.4118\n",
      "Epoch [1600/500000], Loss: 2.6642, BCE_task1; 1.2738, BCE_task2; 1.3904\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.862, ACC_task2 0.138\n",
      "F1_task1 0.926, F1_task2 0.000\n",
      "AUC_task1 0.690, AUC_task2 0.670\n",
      "time : 54.83921790122986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4f065e543b06>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
      "<ipython-input-10-4f065e543b06>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 3.0922, BCE_task1; 1.5513, BCE_task2; 1.5409\n",
      "Epoch [200/500000], Loss: 3.0041, BCE_task1; 1.5211, BCE_task2; 1.4829\n",
      "Epoch [400/500000], Loss: 2.9719, BCE_task1; 1.5114, BCE_task2; 1.4605\n",
      "Epoch [600/500000], Loss: 2.9356, BCE_task1; 1.5016, BCE_task2; 1.4340\n",
      "Epoch [800/500000], Loss: 2.9091, BCE_task1; 1.4934, BCE_task2; 1.4157\n",
      "Early stopping\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ACC_task1 0.138, ACC_task2 0.138\n",
      "F1_task1 0.000, F1_task2 0.000\n",
      "AUC_task1 0.360, AUC_task2 0.450\n",
      "time : 29.47883701324463\n",
      "Task1 AUC: 0.496, Task2 AUC: 0.518\n"
     ]
    }
   ],
   "source": [
    "auc1 = []\n",
    "auc2 = []\n",
    "for fold_ in range(1,6):\n",
    "    x_gen1 = pd.read_csv('./omics1_' + str(fold_) + '_' + '.csv', delimiter=',', index_col=0)\n",
    "    x_gen2 = pd.read_csv('./omics2_' + str(fold_) + '_' + '.csv', delimiter=',', index_col=0)\n",
    "    \n",
    "    train_index = pd.read_csv('./train_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    valid_index = pd.read_csv('./valid_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    test_index = pd.read_csv('./test_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    \n",
    "    train_index = np.array(train_index).reshape(-1).astype(int)\n",
    "    valid_index = np.array(valid_index).reshape(-1).astype(int)\n",
    "    test_index = np.array(test_index).reshape(-1).astype(int)\n",
    "    \n",
    "    Xg_train = pd.concat((x1.transpose().iloc[train_index,:],x_gen1)).values\n",
    "    Xg_valid = x1.transpose().iloc[valid_index,:].values\n",
    "    Xg_test = x1.transpose().iloc[test_index,:].values\n",
    "\n",
    "    Xm_train = pd.concat((x2.transpose().iloc[train_index,:],x_gen2)).values\n",
    "    Xm_valid = x2.transpose().iloc[valid_index,:].values\n",
    "    Xm_test = x2.transpose().iloc[test_index,:].values\n",
    "\n",
    "    y_train = labels[train_index]\n",
    "    y_train = np.concatenate((y_train,y_train))\n",
    "    y_valid = labels[valid_index]\n",
    "    y_test = labels[test_index]\n",
    "    \n",
    "    start = time.time()\n",
    "    earlyStoppingPatience = 100\n",
    "    learningRate = 0.000005\n",
    "    weightDecay = 0.001\n",
    "    num_epochs = 500000 \n",
    "\n",
    "    \n",
    "    y_train = y_train.astype(int)\n",
    "    y_valid = y_valid.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    Xg = torch.tensor(Xg_train, dtype=torch.float32).cuda()\n",
    "    Xm = torch.tensor(Xm_train, dtype=torch.float32).cuda()\n",
    "    \n",
    "    Xg_valid = torch.tensor(Xg_valid, dtype=torch.float32).cuda()\n",
    "    Xm_valid = torch.tensor(Xm_valid, dtype=torch.float32).cuda()\n",
    "    \n",
    "\n",
    "    Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
    "    Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n",
    "\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).cuda()\n",
    "    y_v = torch.tensor(y_valid, dtype=torch.float32).cuda()\n",
    "\n",
    "    ds = TensorDataset(Xg, Xm,y)\n",
    "    dv = TensorDataset(Xg_valid,Xm_valid,y_v)\n",
    "    \n",
    "    loader  = DataLoader(ds, batch_size=y_train.shape[0],shuffle=True)\n",
    "    loader_v = DataLoader(dv, batch_size=y_valid.shape[0],shuffle=True)\n",
    "\n",
    "    \n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    In_Nodes1 = Xg_train.shape[1] \n",
    "    In_Nodes2 = Xm_train.shape[1]\n",
    "\n",
    "    # mtlAttention(In_Nodes1,In_Nodes2, # of module)\n",
    "    net = mtlAttention(In_Nodes1,In_Nodes2,32)\n",
    "    net = net.to(device)\n",
    "    early_stopping = EarlyStopping(patience=earlyStoppingPatience, verbose=False)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    \n",
    "    train_losses =[]\n",
    "    valid_losses = [] \n",
    "    \n",
    "    for epoch in (range(num_epochs)):\n",
    "        running_loss1 = 0.0\n",
    "        running_loss2 = 0.0\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            xg,xm, y = data\n",
    "            output1,output2 = net.forward_one(xg,xm)\n",
    "            output1  = output1.squeeze()\n",
    "            output2  = output2.squeeze()\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output1, y) + loss_fn(output2, y) \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "            running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "            train_losses.append(running_loss1+running_loss2)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for i, data in enumerate(loader_v,0):\n",
    "                xg,xm, y = data\n",
    "                output1,output2 = net.forward_one(xg,xm)\n",
    "                output1  = output1.squeeze()\n",
    "                output2  = output2.squeeze()\n",
    "                loss = loss_fn(output1, y) + loss_fn(output2, y)\n",
    "                running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                valid_losses.append(running_loss1+running_loss2)\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        \n",
    "        early_stopping(valid_loss, net)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            print(\"--------------------------------------------------------------------------------------------------\")\n",
    "            break\n",
    "\n",
    "        if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "            if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.4f}, BCE_task1; {:.4f}, BCE_task2; {:.4f}'.format(epoch+1,num_epochs, running_loss1+running_loss2,running_loss1,running_loss2))\n",
    "\n",
    "    ### Test\n",
    "    best_net = early_stopping.best_model\n",
    "    test1,test2 = best_net.forward_one(Xg_test.clone().detach(),Xm_test.clone().detach())\n",
    "    test1 = test1.cpu().detach().numpy()\n",
    "    test2 = test2.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    print (\"ACC_task1 %.3f, ACC_task2 %.3f\" %(accuracy_score(list(y_test),np.where(test1 > 0.5, 1, 0) ),accuracy_score(list(y_test),np.where(test2 > 0.5, 1, 0))))\n",
    "    print (\"F1_task1 %.3f, F1_task2 %.3f\" %(f1_score(list(y_test),np.where(test1 > 0.5, 1, 0)),f1_score(list(y_test),np.where(test2 > 0.5, 1, 0))))\n",
    "    print (\"AUC_task1 %.3f, AUC_task2 %.3f\" %(roc_auc_score(y_test.reshape(-1),test1),roc_auc_score(y_test.reshape(-1),test2)))\n",
    "    print(\"time :\", time.time() - start)\n",
    "    auc1.append(roc_auc_score(y_test.reshape(-1),test1))\n",
    "    auc2.append(roc_auc_score(y_test.reshape(-1),test2))\n",
    "print(\"Task1 AUC: %.3f, Task2 AUC: %.3f\" %(sum(auc1)/len(auc1), sum(auc2)/len(auc2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list= [16,32,64,128]\n",
    "lr_list = [0.000005,0.00005,0.0005,0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:54<00:00, 14.63s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.560, AUC_task2 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:38<00:00,  9.92s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.350, AUC_task2 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:32<00:00,  9.51s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.610, AUC_task2 0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:46<00:00, 14.14s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.680, AUC_task2 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:47<00:00, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.540, AUC_task2 0.610\n",
      "Task1 AUC: 0.548, Task2 AUC: 0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "auc1 = []\n",
    "auc2 = []\n",
    "for fold_ in range(1,6):\n",
    "    train_index = pd.read_csv('./train_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    valid_index = pd.read_csv('./valid_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    test_index = pd.read_csv('./test_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    \n",
    "    train_index = np.array(train_index).reshape(-1).astype(int)\n",
    "    valid_index = np.array(valid_index).reshape(-1).astype(int)\n",
    "    test_index = np.array(test_index).reshape(-1).astype(int)\n",
    "    \n",
    "    Xg_train = x1.transpose().iloc[train_index,:].values\n",
    "    Xg_valid = x1.transpose().iloc[valid_index,:].values\n",
    "    Xg_test = x1.transpose().iloc[test_index,:].values\n",
    "\n",
    "    Xm_train = x2.transpose().iloc[train_index,:].values\n",
    "    Xm_valid = x2.transpose().iloc[valid_index,:].values\n",
    "    Xm_test = x2.transpose().iloc[test_index,:].values\n",
    "\n",
    "    y_train = labels[train_index]\n",
    "    y_valid = labels[valid_index]\n",
    "    y_test = labels[test_index]\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    earlyStoppingPatience = 100\n",
    "    weightDecay = 0.001\n",
    "    num_epochs = 500000 \n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_valid = y_valid.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    Xg = torch.tensor(Xg_train, dtype=torch.float32).cuda()\n",
    "    Xm = torch.tensor(Xm_train, dtype=torch.float32).cuda()\n",
    "    \n",
    "    Xg_valid = torch.tensor(Xg_valid, dtype=torch.float32).cuda()\n",
    "    Xm_valid = torch.tensor(Xm_valid, dtype=torch.float32).cuda()\n",
    "    \n",
    "\n",
    "    Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
    "    Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n",
    "\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).cuda()\n",
    "    y_v = torch.tensor(y_valid, dtype=torch.float32).cuda()\n",
    "\n",
    "    ds = TensorDataset(Xg, Xm,y)\n",
    "    dv = TensorDataset(Xg_valid,Xm_valid,y_v)\n",
    "    \n",
    "    loader  = DataLoader(ds, batch_size=y_train.shape[0],shuffle=True)\n",
    "    loader_v = DataLoader(dv, batch_size=y_valid.shape[0],shuffle=True)\n",
    "\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    In_Nodes1 = Xg_train.shape[1] \n",
    "    In_Nodes2 = Xm_train.shape[1]\n",
    "    \n",
    "    val_set = list(itertools.product(module_list,lr_list))\n",
    "    best_valid = 99999\n",
    "    for iter_ in tqdm.tqdm(range(0,len(val_set))):\n",
    "        val_par = val_set[iter_]\n",
    "        mod_num = val_par[0]\n",
    "        learningRate = val_par[1]\n",
    "        \n",
    "    \n",
    "        # mtlAttention(In_Nodes1,In_Nodes2, # of module)\n",
    "        net = mtlAttention(In_Nodes1,In_Nodes2,mod_num)\n",
    "        net = net.to(device)\n",
    "        early_stopping = EarlyStopping(patience=earlyStoppingPatience, verbose=False)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "    \n",
    "        train_losses =[]\n",
    "        valid_losses = [] \n",
    "    \n",
    "        for epoch in (range(num_epochs)):\n",
    "            running_loss1 = 0.0\n",
    "            running_loss2 = 0.0\n",
    "            for i, data in enumerate(loader, 0):\n",
    "                xg,xm, y = data\n",
    "                output1,output2 = net.forward_one(xg,xm)\n",
    "                output1  = output1.squeeze()\n",
    "                output2  = output2.squeeze()\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(output1, y) + loss_fn(output2, y) \n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                train_losses.append(running_loss1+running_loss2)\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for i, data in enumerate(loader_v,0):\n",
    "                    xg,xm, y = data\n",
    "                    output1,output2 = net.forward_one(xg,xm)\n",
    "                    output1  = output1.squeeze()\n",
    "                    output2  = output2.squeeze()\n",
    "                    loss = loss_fn(output1, y) + loss_fn(output2, y)\n",
    "                    running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                    running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                    valid_losses.append(running_loss1+running_loss2)\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "            \n",
    "            early_stopping(valid_loss, net)\n",
    "            if early_stopping.early_stop:\n",
    "            #    print(\"Early stopping\")\n",
    "            #    print(\"--------------------------------------------------------------------------------------------------\")\n",
    "                break\n",
    "\n",
    "            #if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "             #   if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "             #       print ('Epoch [{}/{}], Loss: {:.4f}, BCE_task1; {:.4f}, BCE_task2; {:.4f}'.format(epoch+1,num_epochs, running_loss1+running_loss2,running_loss1,running_loss2))\n",
    "            \n",
    "            \n",
    "        best_score = early_stopping.best_score\n",
    "        if best_score < best_valid:\n",
    "            best_valid = valid_loss\n",
    "            save_mod = mod_num\n",
    "            save_lr = learningRate\n",
    "            \n",
    "            model = early_stopping.best_model\n",
    "            \n",
    "            \n",
    "            best_net = early_stopping.best_model\n",
    "            test1,test2 = best_net.forward_one(Xg_test.clone().detach(),Xm_test.clone().detach())\n",
    "            test1 = test1.cpu().detach().numpy()\n",
    "            test2 = test2.cpu().detach().numpy()\n",
    "            \n",
    "            auc1_best = roc_auc_score(y_test.reshape(-1),test1)\n",
    "            auc2_best = roc_auc_score(y_test.reshape(-1),test2)\n",
    "    \n",
    "    print (\"AUC_task1 %.3f, AUC_task2 %.3f\" %(auc1_best,auc2_best))\n",
    "    auc1.append(auc1_best)\n",
    "    auc2.append(auc2_best)\n",
    "print(\"Task1 AUC: %.3f, Task2 AUC: %.3f\" %(sum(auc1)/len(auc1), sum(auc2)/len(auc2)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:18<00:00, 19.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.570, AUC_task2 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:16<00:00, 16.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.360, AUC_task2 0.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:03<00:00, 15.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.700, AUC_task2 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:53<00:00, 18.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.670, AUC_task2 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:03<00:00, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_task1 0.630, AUC_task2 0.610\n",
      "Task1 AUC: 0.586, Task2 AUC: 0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "auc1 = []\n",
    "auc2 = []\n",
    "for fold_ in range(1,6):\n",
    "    \n",
    "    x_gen1 = pd.read_csv('./omics1_' + str(fold_) + '_' + '.csv', delimiter=',', index_col=0)\n",
    "    x_gen2 = pd.read_csv('./omics2_' + str(fold_) + '_' + '.csv', delimiter=',', index_col=0)\n",
    "    \n",
    "    train_index = pd.read_csv('./train_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    valid_index = pd.read_csv('./valid_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    test_index = pd.read_csv('./test_index' + str(fold_) + '.csv', delimiter=',',header=None)\n",
    "    \n",
    "    train_index = np.array(train_index).reshape(-1).astype(int)\n",
    "    valid_index = np.array(valid_index).reshape(-1).astype(int)\n",
    "    test_index = np.array(test_index).reshape(-1).astype(int)\n",
    "    \n",
    "    Xg_train = pd.concat((x1.transpose().iloc[train_index,:],x_gen1)).values\n",
    "    Xg_valid = x1.transpose().iloc[valid_index,:].values\n",
    "    Xg_test = x1.transpose().iloc[test_index,:].values\n",
    "\n",
    "    Xm_train = pd.concat((x2.transpose().iloc[train_index,:],x_gen2)).values\n",
    "    Xm_valid = x2.transpose().iloc[valid_index,:].values\n",
    "    Xm_test = x2.transpose().iloc[test_index,:].values\n",
    "\n",
    "    y_train = labels[train_index]\n",
    "    y_train = np.concatenate((y_train,y_train))\n",
    "    y_valid = labels[valid_index]\n",
    "    y_test = labels[test_index]\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    earlyStoppingPatience = 100\n",
    "    weightDecay = 0.001\n",
    "    num_epochs = 500000 \n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    y_valid = y_valid.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    Xg = torch.tensor(Xg_train, dtype=torch.float32).cuda()\n",
    "    Xm = torch.tensor(Xm_train, dtype=torch.float32).cuda()\n",
    "    \n",
    "    Xg_valid = torch.tensor(Xg_valid, dtype=torch.float32).cuda()\n",
    "    Xm_valid = torch.tensor(Xm_valid, dtype=torch.float32).cuda()\n",
    "    \n",
    "\n",
    "    Xg_test = torch.tensor(Xg_test, dtype=torch.float32).cuda()\n",
    "    Xm_test = torch.tensor(Xm_test, dtype=torch.float32).cuda()\n",
    "\n",
    "    y = torch.tensor(y_train, dtype=torch.float32).cuda()\n",
    "    y_v = torch.tensor(y_valid, dtype=torch.float32).cuda()\n",
    "\n",
    "    ds = TensorDataset(Xg, Xm,y)\n",
    "    dv = TensorDataset(Xg_valid,Xm_valid,y_v)\n",
    "    \n",
    "    loader  = DataLoader(ds, batch_size=y_train.shape[0],shuffle=True)\n",
    "    loader_v = DataLoader(dv, batch_size=y_valid.shape[0],shuffle=True)\n",
    "\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    In_Nodes1 = Xg_train.shape[1] \n",
    "    In_Nodes2 = Xm_train.shape[1]\n",
    "    \n",
    "    val_set = list(itertools.product(module_list,lr_list))\n",
    "    best_valid = 99999\n",
    "    for iter_ in tqdm.tqdm(range(0,len(val_set))):\n",
    "        val_par = val_set[iter_]\n",
    "        mod_num = val_par[0]\n",
    "        learningRate = val_par[1]\n",
    "        \n",
    "    \n",
    "        # mtlAttention(In_Nodes1,In_Nodes2, # of module)\n",
    "        net = mtlAttention(In_Nodes1,In_Nodes2,mod_num)\n",
    "        net = net.to(device)\n",
    "        early_stopping = EarlyStopping(patience=earlyStoppingPatience, verbose=False)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "    \n",
    "        train_losses =[]\n",
    "        valid_losses = [] \n",
    "    \n",
    "        for epoch in (range(num_epochs)):\n",
    "            running_loss1 = 0.0\n",
    "            running_loss2 = 0.0\n",
    "            for i, data in enumerate(loader, 0):\n",
    "                xg,xm, y = data\n",
    "                output1,output2 = net.forward_one(xg,xm)\n",
    "                output1  = output1.squeeze()\n",
    "                output2  = output2.squeeze()\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(output1, y) + loss_fn(output2, y) \n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                train_losses.append(running_loss1+running_loss2)\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for i, data in enumerate(loader_v,0):\n",
    "                    xg,xm, y = data\n",
    "                    output1,output2 = net.forward_one(xg,xm)\n",
    "                    output1  = output1.squeeze()\n",
    "                    output2  = output2.squeeze()\n",
    "                    loss = loss_fn(output1, y) + loss_fn(output2, y)\n",
    "                    running_loss1 += loss_fn(output1,y.view(-1)).item()\n",
    "                    running_loss2 += loss_fn(output2,y.view(-1)).item()\n",
    "                    valid_losses.append(running_loss1+running_loss2)\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "            \n",
    "            early_stopping(valid_loss, net)\n",
    "            if early_stopping.early_stop:\n",
    "            #    print(\"Early stopping\")\n",
    "            #    print(\"--------------------------------------------------------------------------------------------------\")\n",
    "                break\n",
    "\n",
    "            #if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "             #   if (epoch+1) % 200 == 0 or epoch == 0:\n",
    "             #       print ('Epoch [{}/{}], Loss: {:.4f}, BCE_task1; {:.4f}, BCE_task2; {:.4f}'.format(epoch+1,num_epochs, running_loss1+running_loss2,running_loss1,running_loss2))\n",
    "            \n",
    "            \n",
    "        best_score = early_stopping.best_score\n",
    "        if best_score < best_valid:\n",
    "            best_valid = valid_loss\n",
    "            save_mod = mod_num\n",
    "            save_lr = learningRate\n",
    "            \n",
    "            model = early_stopping.best_model\n",
    "            \n",
    "            \n",
    "            best_net = early_stopping.best_model\n",
    "            test1,test2 = best_net.forward_one(Xg_test.clone().detach(),Xm_test.clone().detach())\n",
    "            test1 = test1.cpu().detach().numpy()\n",
    "            test2 = test2.cpu().detach().numpy()\n",
    "            \n",
    "            auc1_best = roc_auc_score(y_test.reshape(-1),test1)\n",
    "            auc2_best = roc_auc_score(y_test.reshape(-1),test2)\n",
    "    \n",
    "    print (\"AUC_task1 %.3f, AUC_task2 %.3f\" %(auc1_best,auc2_best))\n",
    "    auc1.append(auc1_best)\n",
    "    auc2.append(auc2_best)\n",
    "print(\"Task1 AUC: %.3f, Task2 AUC: %.3f\" %(sum(auc1)/len(auc1), sum(auc2)/len(auc2)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
